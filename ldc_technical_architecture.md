# Life Decision Coach (LDC) - Complete Technical Architecture

> [!IMPORTANT]
> **This is NOT an advice system. This is decision infrastructure.**  
> LDC computes reachable future life trajectories using real data, hard constraints, and learned transition patterns. LLMs are restricted to explanation only.

---

## Executive Summary

**Life Decision Coach** is a data-driven AI system that helps users understand long-term consequences of high-stakes decisions (career, education, finance, migration, entrepreneurship) by computing **reachable future life trajectories** rather than giving advice.

### Core Differentiators (v3.0)
- âœ… **Data-first, not LLM-first**: All decision logic comes from historical data
- âœ… **Constraint-aware with temporal reasoning**: Models hard reality + time-dependent rules
- âœ… **Uncertainty-honest**: Shows P10/P50/P90 futures across multiple time horizons
- âœ… **Deep learning powered**: Transformers, GNNs, ensemble forecasting
- âœ… **Causal reasoning**: Not just correlation, but understanding cause-effect
- âœ… **Lock-in & regret aware**: Integrated path analysis (reversibility, opportunity cost)
- âœ… **Adaptive explanations**: Expert/novice modes + visual charts
- âŒ **Not a chatbot**: No conversational advice
- âŒ **Not a recommender**: No "you should do X"
- âŒ **Not a simulation**: Deterministic outcome envelopes, not Monte Carlo

---

## System Architecture: 4-Layer Stack (Upgraded)

> [!NOTE]
> **Architecture v3.0**: Removed Layer 4 as standalone, redistributed functionality across layers, upgraded with modern ML/AI techniques.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Multi-Level Explanation (UPGRADED)                â”‚
â”‚  Purpose: Adaptive explanations + visual generation          â”‚
â”‚  Features: Expert/novice modes, charts, interactive Q&A      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 3: Advanced Trajectory Intelligence (UPGRADED)       â”‚
â”‚  Purpose: Deep learning + ensemble + counterfactual          â”‚
â”‚  â”œâ”€ 3A: Multi-Modal Trajectory Graph (embeddings, GNNs)     â”‚
â”‚  â”œâ”€ 3B: Ensemble Forecasting (Transformers, GB, GP)         â”‚
â”‚  â”œâ”€ 3C: Causal RAG with Attribution (not just correlation)  â”‚
â”‚  â””â”€ 3D: Multi-Horizon Envelopes + Regret Analysis           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 2: Intelligent Constraint & Path Analysis (UPGRADED) â”‚
â”‚  â”œâ”€ 2A: Temporal Constraint Graph (time-dependent rules)    â”‚
â”‚  â”œâ”€ 2B: LLM Classifier + Lock-In Detector (integrated)      â”‚
â”‚  â””â”€ 2C: Reversibility & Path Properties (NEW sublayer)      â”‚
â”‚  Purpose: Smart filtering + path quality scoring             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Layer 1: Adaptive State Ingestion (UPGRADED)               â”‚
â”‚  Purpose: Multi-modal input + progressive disclosure         â”‚
â”‚  Features: LinkedIn import, voice input, privacy-preserving  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Changed in v3.0

**Removed**: Layer 4 (Risk, Regret & Lock-In Analysis) as standalone layer

**Redistributed**:
- Lock-in detection â†’ Layer 2B (it's constraint graph topology)
- Regret computation â†’ Layer 3D (it's trajectory comparison)
- Reversibility scoring â†’ Layer 2C (new sublayer for path properties)

**Upgraded**: Every layer with cutting-edge ML/AI techniques



---

##Layer 1: Adaptive State Ingestion (UPGRADED v3.0)

> [!NOTE]
> **v3.0 Upgrades**: Multi-modal input processing, progressive disclosure, privacy-preserving data handling, schema versioning

### Purpose
Convert diverse user inputs into a **structured, machine-readable Life State Vector** through adaptive, multi-modal ingestion that minimizes user effort while maximizing data quality.

### Technical Implementation

#### Input Sources
1. **Structured Questionnaire**
   - Age, education level, current location, work experience
   - Financial resources (savings, debt, income)
   - Immigration status (visa type, expiry dates)
   - Family constraints (dependents, caregiving)
   - Risk tolerance (measured via decision scenarios)

2. **Document Upload & Parsing**
   - Resume/CV â†’ extract skills, certifications, work history
   - Transcripts â†’ extract GPA, degree type, graduation date
   - Financial statements â†’ extract net worth, income streams

3. **Validation & Normalization**
   - Date validation (e.g., visa expiry must be future)
   - Geographic normalization (city â†’ country â†’ region)
   - Skill taxonomy mapping (user's "React" â†’ canonical "React.js v18+")

#### Output: Life State Vector

```python
LifeState = {
    # Demographics
    "age": 28,
    "location": "Mumbai, India",
    "citizenship": ["India"],
    "visa_status": {"US": {"type": "F1", "expiry": "2027-06-30"}},
    
    # Education
    "education": [
        {"degree": "B.Tech", "field": "CS", "gpa": 3.7, "year": 2020}
    ],
    
    # Career
    "work_history": [
        {"role": "SWE", "company": "TCS", "years": 3, "industry": "IT"}
    ],
    "skills": ["Python", "React", "AWS"],
    "certifications": ["AWS Solutions Architect"],
    
    # Financial
    "savings": 15000,  # USD
    "debt": 0,
    "income": 25000,  # annual in USD
    
    # Constraints
    "dependents": 0,
    "health_constraints": [],
    "risk_tolerance": 0.7  # 0-1 scale
}
```

#### Dynamic State Updates

The Life State Vector is **mutable** and progressively enriched throughout the system pipeline:

- **Layer 1 (Initial)**: Base ingestion from structured questionnaire and document parsing
- **Layer 2B (Interactive)**: Enhanced with missing information from clarifying questions
- **Layer 3+**: State frozen for consistent trajectory modeling

**Example: State Evolution**

```python
# Initial state after Layer 1
initial_state = {
    "age": 28,
    "education": [{"degree": "B.Tech", "field": "CS", "gpa": 3.7}],
    "location": "Mumbai, India",
    # work_experience: MISSING
    # language_scores: MISSING
}

# Enhanced state after Layer 2B interaction
enriched_state = {
    "age": 28,
    "education": [{"degree": "B.Tech", "field": "CS", "gpa": 3.7}],
    "location": "Mumbai, India",
    "work_experience": [  # NEW - from clarification
        {"role": "SWE", "company": "TCS", "years": 3, "noc_code": "2173"}
    ],
    "language_scores": {  # NEW - from clarification
        "ielts": {"listening": 8.5, "reading": 8.0, "writing": 7.5, "speaking": 7.5}
    }
}
```

This dynamic enrichment allows the system to:
- Start with minimal user effort (basic questionnaire)
- Request additional details only when needed for specific paths
- Avoid overwhelming users with unnecessary questions upfront

#### Why No Free-Text?
- Structured data enables **deterministic constraint checking**
- Removes LLM hallucination risk in critical inputs
- Enables **efficient graph traversal** in later layers

---

## Layer 2: Intelligent Constraint & Path Analysis (UPGRADED v3.0)

> [!NOTE]
> **v3.0 Upgrades**: Added Layer 2C (path properties), integrated lock-in detection into 2B, temporal constraint support in 2A

Layer 2 has three sublayers that work sequentially:
- **Layer 2A**: Temporal constraint graph (time-dependent rules)
- **Layer 2B**: LLM classifier + lock-in detector (integrated from old Layer 4)
- **Layer 2C**: Reversibility & path properties (NEW - quality scoring)

---

### Layer 2A: Hard Constraint Graph

#### Purpose
Encode **hard structural constraints** that make certain futures physically/legally impossible.

### Graph Structure

#### Node Types
1. **Credential Nodes**: Degrees, certifications, licenses
2. **Legal Nodes**: Visas, work permits, citizenship
3. **Life Event Nodes**: Age milestones (e.g., "under 30 for Canada Express Entry")
4. **Financial Nodes**: Minimum savings thresholds (e.g., "$50k for US EB-5 visa")

#### Edge Types
1. **Prerequisite Edges**: `A â†’ B` means "B requires A"
   - Example: `"US H1B visa" â† "Bachelor's degree or equivalent"`
2. **Exclusion Edges**: `A âŠ— B` means "cannot have both"
   - Example: `"US Green Card" âŠ— "India citizenship" (must renounce)`
3. **Time Window Edges**: `A â†’[t1, t2] B` means "B only possible in time window"
   - Example: `"German Job Seeker Visa" â†’[age 18-40] "Apply"`

#### Example Subgraph: US Immigration Paths

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Bachelor's   â”‚
          â”‚ Degree       â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚ H1B    â”‚      â”‚ F1 OPT  â”‚
    â”‚ Visa   â”‚      â”‚ (1 year)â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚                â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚  â”‚
    â”Œâ”€â”€â”€â–¼â”€â”€â–¼â”€â”€â”€â”
    â”‚ Green    â”‚
    â”‚ Card     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ (EB-2/3) â”‚                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ EB-5 Visa   â”‚â—„â”€â”€â”€â”€â”‚ $800k+ savings â”‚
    â”‚ (investor)  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Constraint Enforcement Algorithm

```python
def is_feasible(current_state: LifeState, target_state: LifeState) -> bool:
    """Check if target state is reachable given hard constraints"""
    
    # Check age windows
    if target_state.requires_age_range():
        if not target_state.age_range.contains(current_state.age):
            return False
    
    # Check credential prerequisites
    required_creds = get_prerequisites(target_state)
    if not all(cred in current_state.education for cred in required_creds):
        return False
    
    # Check financial thresholds
    if target_state.min_savings > current_state.savings:
        return False
    
    # Check visa/legal constraints
    if target_state.requires_visa:
        if not current_state.has_valid_visa(target_state.location):
            return False
    
    return True
```

### Why This Matters
- **Cuts search space by 80%+**: No wasted compute on impossible futures
- **Prevents misleading outputs**: Never suggests "move to US" if visa is impossible
- **Legally defensible**: All rules traceable to government regulations


### Why This Matters
- **Cuts search space by 80%+**: No wasted compute on impossible futures
- **Prevents misleading outputs**: Never suggests "move to US" if visa is impossible
- **Legally defensible**: All rules traceable to government regulations

---

### Layer 2B: Interactive Feasibility Classification

#### Purpose
Use LLM to intelligently categorize paths that pass hard constraints (Layer 2A) into:
1. **Completely Feasible** - All information available, proceed to Layer 3
2. **Maybe Feasible** - Missing information, ask clarifying questions
3. Paths that fail Layer 2A are already **Completely Unfeasible** (rejected)

#### Three-Tier Classification System

```python
def classify_path_feasibility(path, user_state, constraint_graph):
    """
    LLM classifies a candidate path after hard constraints are verified
    
    Returns: Classification status + reasoning
    """
    
    # Build classification prompt for LLM
    prompt = f"""
    You are a feasibility classifier for life decision paths.
    
    USER STATE:
    {json.dumps(user_state, indent=2)}
    
    CANDIDATE PATH:
    Path: {path.name}
    Description: {path.description}
    
    HARD CONSTRAINTS (ALREADY VERIFIED AS PASSING):
    {path.satisfied_constraints}
    
    TASK:
    Determine if we have ALL necessary information to model this path's 
    trajectory, or if critical data is missing.
    
    Classify as:
    - FEASIBLE: All required information is present in user state
    - MAYBE: Missing critical information needed for trajectory modeling
    
    If MAYBE, specify EXACTLY what information is needed and WHY.
    
    OUTPUT FORMAT (JSON):
    {{
        "status": "FEASIBLE" or "MAYBE",
        "confidence": 0.0-1.0,
        "reasoning": "brief explanation",
        "missing_info": [  // only if MAYBE
            {{
                "field": "work_experience_years",
                "question": "How many years of continuous work experience do you have?",
                "why_needed": "Required for Canada Express Entry CRS score calculation"
            }}
        ]
    }}
    
    CONSTRAINTS:
    - Base decisions on documented requirements only (cite sources)
    - Do NOT invent arbitrary requirements
    - Only ask for information that DIRECTLY impacts feasibility
    - Maximum 3 clarifying questions per path
    """
    
    llm_response = call_llm_with_constraints(prompt)
    return parse_json(llm_response)
```

#### Interactive Clarification Loop

```python
def refine_feasibility_through_interaction(user_state, maybe_paths):
    """
    For each 'MAYBE' path, ask user clarifying questions and re-classify
    
    Returns: Updated paths + enriched user state
    """
    
    refined_paths = {
        "feasible": [],
        "unfeasible": []
    }
    
    updated_state = user_state.copy()
    
    for path in maybe_paths:
        classification = path.llm_classification
        
        # Extract questions from LLM classification
        questions = classification["missing_info"]
        
        # Present questions to user
        print(f"\nğŸ“‹ Path: {path.name}")
        print(f"â„¹ï¸  {classification['reasoning']}\n")
        
        user_answers = {}
        for q in questions:
            answer = input(f"â“ {q['question']}: ")
            user_answers[q['field']] = answer
            print(f"   ğŸ’¡ {q['why_needed']}\n")
        
        # Merge answers into user state
        updated_state = merge_user_state(updated_state, user_answers)
        
        # Re-check hard constraints with new information
        if violates_hard_constraints(path, updated_state):
            refined_paths["unfeasible"].append(path)
            print(f"âŒ Path blocked: New information reveals constraint violation\n")
        else:
            refined_paths["feasible"].append(path)
            print(f"âœ… Path unlocked: All requirements satisfied\n")
    
    return refined_paths, updated_state
```

#### Example: Canada Express Entry Path

**Scenario**: User is 28, has B.Tech in CS, no work experience info provided

**Step 1: Hard Constraint Check (Layer 2A)**
```python
# Canada Express Entry requirements
required_constraints = {
    "age": "< 35 years",           # âœ“ User is 28
    "education": "Bachelor's+",    # âœ“ User has B.Tech
}

# All hard constraints pass â†’ proceed to Layer 2B
```

**Step 2: LLM Classification (Layer 2B)**
```json
{
  "status": "MAYBE",
  "confidence": 0.85,
  "reasoning": "Express Entry uses Comprehensive Ranking System (CRS) which requires work experience duration and language test scores for calculation. Missing these critical inputs.",
  "missing_info": [
    {
      "field": "work_experience_years",
      "question": "How many years of continuous skilled work experience do you have (NOC 0, A, or B)?",
      "why_needed": "CRS awards up to 50 points for work experience. Required to estimate if you'll exceed typical cutoff (450+ points)."
    },
    {
      "field": "language_test_scores",
      "question": "Do you have IELTS, CELPIP, or TEF scores? If yes, what are they?",
      "why_needed": "Language proficiency is mandatory for Express Entry and accounts for up to 136 CRS points. Without this, cannot compute eligibility."
    },
    {
      "field": "canadian_job_offer",
      "question": "Do you have a valid job offer from a Canadian employer with LMIA?",
      "why_needed": "Job offer adds 50-200 CRS points depending on NOC level. Significantly impacts competitiveness."
    }
  ]
}
```

**Step 3: User Interaction**
```
ğŸ“‹ Path: Canada Express Entry (Federal Skilled Worker)
â„¹ï¸  Express Entry uses CRS scoring. Need work history and language scores to estimate eligibility.

â“ How many years of continuous skilled work experience do you have (NOC 0, A, or B)?: 3
   ğŸ’¡ CRS awards up to 50 points for work experience. Required to estimate if you'll exceed typical cutoff (450+ points).

â“ Do you have IELTS, CELPIP, or TEF scores? If yes, what are they?: IELTS - L:8.5 R:8.0 W:7.5 S:7.5
   ğŸ’¡ Language proficiency is mandatory for Express Entry and accounts for up to 136 CRS points. Without this, cannot compute eligibility.

â“ Do you have a valid job offer from a Canadian employer with LMIA?: No
   ğŸ’¡ Job offer adds 50-200 CRS points depending on NOC level. Significantly impacts competitiveness.
```

**Step 4: State Update & Re-Classification**
```python
# Updated user state
enriched_state = {
    "age": 28,
    "education": [{"degree": "B.Tech", "field": "CS"}],
    "work_experience": [{"years": 3, "noc_code": "2173"}],  # NEW
    "language_scores": {  # NEW
        "ielts": {"L": 8.5, "R": 8.0, "W": 7.5, "S": 7.5}
    },
    "canadian_job_offer": False  # NEW (explicit)
}

# Estimated CRS = 470-490 (age + education + experience + language)
# Typical cutoff = 450-460

# Result: âœ… Path classified as FEASIBLE
```

**Step 5: Path Proceeds to Layer 3**
```
âœ… Path unlocked: All requirements satisfied

Path now moves to Layer 3 (Trajectory Intelligence) with enriched user state.
Trajectory models will use:
- 3 years work experience â†’ affects transition probabilities
- IELTS CLB 9-10 â†’ improves visa approval likelihood
- No job offer â†’ models "direct Express Entry" pathway (not PNP)
```

---

#### Benefits of Interactive Classification

**1. Reduces False Negatives**
Traditional systems reject paths when data is missing. LDC asks clarifying questions, converting "insufficient data" into actionable insights.

- **Old behavior**: User doesn't mention IELTS â†’ Canada path blocked
- **New behavior**: System asks "Do you have IELTS?" â†’ User provides score â†’ Path unlocked

**2. Improves Trajectory Modeling Accuracy**
Interactive clarification enriches the Life State Vector with targeted, high-value information that directly impacts probability calculations in Layer 3.

**3. Educational & Transparent**
Each question includes a "why this matters" explanation, helping users understand what requirements exist and why they're critical.

**4. Maintains System Integrity**
Unlike unconstrained LLMs, Layer 2B:
- Only operates on paths that passed hard constraints (Layer 2A)
- Cannot invent requirements (must cite official sources)
- Outputs structured JSON (no free-form advice)
- All questions logged and auditable

---

#### LLM Constraints in Layer 2B

> [!IMPORTANT]
> **Strict Guardrails for LLM Classifier**

1. **Read-Only State Access**: LLM can read user state and constraint graph but cannot modify them
2. **Binary Classification**: Must output `FEASIBLE` or `MAYBE` (no recommendations like "you should apply")
3. **Evidence-Based Questions**: Every question must cite specific requirements (e.g., "CRS requires X per official guide")
4. **Confidence Thresholds**: If LLM confidence < 0.7, default to `MAYBE` (conservative approach)
5. **Question Limits**: Maximum 3 clarifying questions per path (prevents user fatigue)
6. **Audit Logging**: All LLM calls logged with prompts, responses, and timestamps

**Example of BLOCKED behavior**:
```
âŒ VIOLATION: LLM suggests "You should improve your IELTS score to 8.0+"
âœ“  CORRECT: LLM asks "What is your IELTS score?" (neutral, fact-gathering)

âŒ VIOLATION: LLM invents requirement "Need 5+ years for Express Entry"
âœ“  CORRECT: LLM cites "Express Entry CRS awards points for 1-3+ years of experience"
```

---

#### Integration with Layer 3

After Layer 2B classification, the system has:
- **Feasible paths**: Fully specified, ready for trajectory modeling
- **Enriched user state**: Contains all information needed for accurate predictions
- **Explicit rejections**: Unfeasible paths blocked with clear reasoning

**Data Flow to Layer 3**:
```
Layer 2B Output â†’ Layer 3A Input

{
    "user_state": enriched_state,        # Updated with clarification answers
    "feasible_paths": [path1, path2],     # Only paths with complete info
    "interaction_log": [                  # For audit trail
        {"path": "Canada EE", "questions_asked": 3, "answers_provided": 3}
    ]
}
```

Layer 3 can now confidently model trajectories because:
- All hard constraints are satisfied (Layer 2A guarantee)
- All critical information is present (Layer 2B guarantee)
- User state is consistent and complete

---

---

## Layer 3: Advanced Trajectory Intelligence (UPGRADED v3.0)`r`n`r`n> [!NOTE]`r`n> **v3.0 Upgrades**: Ensemble forecasting (Transformers, GNNs), causal RAG, multi-horizon modeling, integrated regret analysis from old Layer 4`r`n

### 3A. Trajectory Knowledge Graph

#### Purpose
Represent **empirically observed** career/life transitions learned from longitudinal data.

#### Data Sources
1. **Longitudinal Surveys**
   - Panel Study of Income Dynamics (PSID) - 50+ years of US family data
   - Survey of Health, Ageing and Retirement in Europe (SHARE)
   - India Human Development Survey (IHDS)

2. **Job Market Data**
   - LinkedIn Career Trajectories (if accessible via API)
   - Indeed job postings + salary data
   - H1B visa salary database (public records)

3. **Immigration Data**
   - USCIS approval rates by visa type
   - Express Entry draws (Canada)
   - EU Blue Card statistics

#### Graph Structure

**Nodes**: Career/Economic States
```python
StateNode = {
    "id": "SWE_L4_BigTech_US",
    "definition": {
        "role": "Software Engineer",
        "level": "L4 (Mid-level)",
        "industry": "BigTech",
        "location": "US",
        "typical_salary": [120000, 180000],  # P25-P75
        "typical_age": [26, 32]
    },
    "sample_size": 4782  # observations in dataset
}
```

**Edges**: Observed Transitions
```python
TransitionEdge = {
    "from": "SWE_L3_Startup_India",
    "to": "SWE_L4_BigTech_US",
    "observations": 237,
    "median_time": 2.3,  # years
    "success_rate": 0.18,  # 18% of attempts
    "typical_path": [
        "Apply to FAANG",
        "Get H1B sponsorship",
        "Relocate"
    ]
}
```

#### Example Subgraph: Software Engineering Trajectories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SWE Intern      â”‚
â”‚ India           â”‚
â”‚ Salary: $8k     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 85% (1.5yr)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SWE L3          â”‚       â”‚ SWE L3          â”‚
â”‚ India Startup   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ BigTech India   â”‚
â”‚ Salary: $15k    â”‚ 22%   â”‚ Salary: $25k    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (2yr) â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                          â”‚
         â”‚ 18% (2.3yr)             â”‚ 31% (1.8yr)
         â–¼                          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ SWE L4          â”‚       â”‚ SWE L5          â”‚
    â”‚ BigTech US      â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ BigTech India   â”‚
    â”‚ Salary: $150k   â”‚ 12%   â”‚ Salary: $40k    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (3yr) â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3B. Empirical Transition Models

#### Purpose
Model **when** and **with what probability** transitions occur, using survival analysis and Bayesian methods.

#### Techniques

**1. Survival Analysis (Time-to-Transition)**

```python
# Kaplan-Meier estimator for "time to promotion"
from lifelines import KaplanMeierFitter

kmf = KaplanMeierFitter()
kmf.fit(durations=time_to_promotion, event_observed=got_promoted)

# Output: P(promoted by time t | still in role)
P_promoted_by_3_years = kmf.survival_function_at_times(3)  # 0.42
```

**2. Cox Proportional Hazards (Context-Conditional Transitions)**

```python
# How does age, location, prior experience affect transition speed?
from lifelines import CoxPHFitter

cph = CoxPHFitter()
cph.fit(df, duration_col='years_to_next_state', event_col='transitioned',
        formula='age + location + prior_experience + has_degree')

# Hazard ratio: degree holders transition 1.8x faster
```

**3. Bayesian Transition Probabilities**

```python
# P(transition | context) with uncertainty quantification
import pymc as pm

with pm.Model() as model:
    # Prior: historical transition rate
    p_base = pm.Beta('p_base', alpha=18, beta=82)  # 18% historical
    
    # Context modifiers
    age_effect = pm.Normal('age_effect', mu=0, sigma=0.1)
    degree_effect = pm.Normal('degree_effect', mu=0.2, sigma=0.05)
    
    # Posterior: adjusted probability
    p_transition = pm.Deterministic('p_transition',
        pm.math.invlogit(pm.math.logit(p_base) + age_effect*age + degree_effect*has_degree))
    
    # Inference
    trace = pm.sample(2000)
    
# Output: P(transition) = 0.23 Â± 0.04 (95% CI)
```

#### Why Not Random Simulation?
- **Preserves uncertainty**: Shows P10/P50/P90, not single "average" path
- **Data-grounded**: Every probability comes from observed data
- **Auditable**: Can trace back to specific cohorts in dataset

---

### 3C. RAG-Augmented Context Modulation

#### Purpose
Inject **real-time external context** (policy changes, economic shocks, regulation updates) to adjust transition probabilities WITHOUT giving RAG decision power.

#### How It Works

**Step 1: Retrieve Relevant Context**
```python
# User wants to predict: "SWE India â†’ SWE US" in 2026
query = "US H1B visa approval rates 2026, tech layoffs, immigration policy"

# RAG retrieves from knowledge base
contexts = vector_db.similarity_search(query, k=5)

# Example retrieved docs:
# - "H1B denial rates increased to 24% in 2025 (USCIS data)"
# - "Tech layoffs: 150k+ in 2023-2024 (TechCrunch)"
# - "New bill proposes H1B cap increase to 110k (pending approval)"
```

**Step 2: LLM Extracts Structured Context**
```python
# LLM input: Retrieved documents
# LLM output: Structured risk modifiers (NOT decision)

context_modifiers = {
    "visa_difficulty_multiplier": 1.3,  # 30% harder due to denials
    "job_availability_multiplier": 0.8,  # 20% fewer openings
    "time_delay_months": +6,  # slower processing
    "confidence": 0.75  # LLM confidence in extraction
}
```

**Step 3: Apply Modifiers to Empirical Model**
```python
# Original transition probability (from Layer 3B)
p_base = 0.18  # 18% historical success rate

# Apply context modifiers
p_adjusted = p_base * (
    context_modifiers["visa_difficulty_multiplier"] ** -1 *
    context_modifiers["job_availability_multiplier"]
)
# p_adjusted = 0.18 * (1/1.3) * 0.8 = 0.11 (11% in current climate)

# Adjust time estimate
time_median_adjusted = time_median_base + context_modifiers["time_delay_months"] / 12
# 2.3 years â†’ 2.8 years
```

#### Guardrails
- âœ… **RAG ONLY modifies parameters, never decides outcomes**
- âœ… **All modifiers logged and auditable**
- âœ… **If LLM confidence < 0.6, use base model only**
- âŒ **RAG NEVER generates new transitions not in knowledge graph**

---

### 3D. Deterministic Outcome Envelopes

#### Purpose
Replace random Monte Carlo simulation with **deterministic P10/P50/P90 futures** that preserve uncertainty without randomness.

#### Concept: Outcome Envelopes
Instead of running 1000 random simulations, compute **best-case / median / worst-case** trajectories using quantile regression.

#### Example: Predicting Net Worth in 5 Years

**Traditional (Bad) Approach: Monte Carlo**
```python
# Run 1000 random simulations
for i in range(1000):
    net_worth[i] = simulate_random_path(current_state, years=5)

# Output: histogram, user sees "median $85k, but could be $20k-$200k"
# Problem: User can't plan around randomness
```

**LDC Approach: Deterministic Envelopes**
```python
# Compute P10, P50, P90 trajectories deterministically

trajectories = {
    "P10_pessimistic": {
        "path": "Stay in current role, no promotion, recession",
        "net_worth_year_5": 32000,
        "assumptions": ["No promotion", "2% salary growth", "Market crash"]
    },
    
    "P50_median": {
        "path": "One promotion, stable market",
        "net_worth_year_5": 85000,
        "assumptions": ["Promotion in year 3", "4% salary growth", "Normal market"]
    },
    
    "P90_optimistic": {
        "path": "Move to US, L5 promotion, equity gains",
        "net_worth_year_5": 220000,
        "assumptions": ["US visa approved", "L5 in year 4", "Stock growth 15%"]
    }
}
```

#### Technical Implementation: Quantile Regression

```python
from sklearn.ensemble import GradientBoostingRegressor

# Train three models: one per quantile
models = {}
for quantile in [0.1, 0.5, 0.9]:
    models[quantile] = GradientBoostingRegressor(
        loss='quantile',
        alpha=quantile,
        n_estimators=100
    )
    models[quantile].fit(X_train, y_train)

# Predict for user's state
X_user = vectorize(current_life_state)
outcomes = {
    "P10": models[0.1].predict(X_user),
    "P50": models[0.5].predict(X_user),
    "P90": models[0.9].predict(X_user)
}
```

#### Pareto Frontier: Non-Dominated Paths

Compute **dominated vs non-dominated** decision paths.

```python
# User has 3 options: A, B, C
# Compare on: (income, work-life balance, career stability)

options = [
    {"name": "A: Startup", "income": 40k, "balance": 3, "stability": 4},
    {"name": "B: BigTech", "income": 150k, "balance": 6, "stability": 8},
    {"name": "C: Govt Job", "income": 50k, "balance": 9, "stability": 10}
]

# Pareto analysis: B dominates A (higher on all dimensions)
# C is non-dominated (best balance + stability, even if lower income)

non_dominated = ["B: BigTech", "C: Govt Job"]
```

#### Why This Matters
- **Plannable**: User can act on "worst-case I need $32k savings buffer"
- **Honest**: Shows uncertainty without pretending to predict randomness
- **Defensible**: Each percentile traceable to specific cohorts in data

---



## Layer 4: Multi-Level Explanation (UPGRADED v3.0)

### Purpose
Translate **numerical outputs** from Layers 1-3 into **human-understandable explanations** WITHOUT allowing LLMs to make decisions.

### Input to LLM
```json
{
  "user_current_state": {
    "age": 28,
    "location": "Mumbai",
    "role": "SWE L3",
    "savings": "$15k"
  },
  
  "decision_under_consideration": "Pursue MS in CS (USA)",
  
  "computed_trajectories": {
    "P10": {"net_worth_year_10": "$45k", "path": "Visa rejected, return to India"},
    "P50": {"net_worth_year_10": "$180k", "path": "MS â†’ H1B â†’ L5 SWE"},
    "P90": {"net_worth_year_10": "$420k", "path": "MS â†’ FAANG L6 â†’ Green Card"}
  },
  
  "risk_analysis": {
    "reversibility_score": 0.3,
    "locked_out_paths": ["Indian startup founder", "early retirement"],
    "median_regret_if_decline": "+$120k"
  },
  
  "context_modifiers": {
    "visa_difficulty": 1.3,
    "job_market": 0.8
  }
}
```

### LLM Prompt (Constrained)
```
You are an explanation engine for a life decision analysis system.

USER DATA (COMPUTED BY DATA MODELS):
{JSON above}

YOUR TASK:
1. Explain the three outcome envelopes (P10/P50/P90) in plain English
2. Highlight the irreversible nature of this decision (reversibility score 0.3)
3. Explain what paths get locked out
4. Explain the regret of NOT doing this (median +$120k)
5. Explain current context (visa difficulty up 30%)

STRICT RULES:
- DO NOT recommend a decision ("you should do X")
- DO NOT add your own analysis beyond explaining the numbers
- DO NOT generate new probabilities or outcomes
- DO NOT use phrases like "I think" or "in my opinion"
- ONLY translate numbers â†’ language

OUTPUT FORMAT:
- Use clear headers
- Use bullet points for lists
- Explain probabilities in plain terms ("1 in 10 chance" not "10%")
```

### Example LLM Output

```
## Outcome Scenarios (Based on Historical Data)

In the **median case** (what happened to 50% of people like you):
- You complete your MS, get an H1B visa, and reach Senior SWE (L5) level
- Your net worth in 10 years: **$180,000**
- This assumes normal job market and visa approval within 2 years

In the **worst case** (what happened to the bottom 10%):
- Visa application is rejected or delayed beyond OPT expiration
- You return to India after MS, resume SWE career there
- Your net worth in 10 years: **$45,000**

In the **best case** (what happened to the top 10%):
- You join a FAANG company, reach L6, get stock appreciation + Green Card
- Your net worth in 10 years: **$420,000**

---

## What This Decision Locks Out

This path has a **reversibility score of 0.3** (where 0 = irreversible, 1 = fully reversible).

If you commit to the MS path, these options become very difficult:
- **Starting a company in India** (locked out for 2-4 years while studying)
- **Early retirement plans** (MS adds debt, delays income)

---

## Cost of NOT Doing This

If you decline the MS and stay in your current role, historical data suggests:
- You would have **$120,000 less** net worth at the median (P50) case
- You would have **$350,000 less** in the best case (P90)

This is based on salary trajectories of people who stayed in India vs those who did MS in US.

---

## Current External Factors

Based on recent policy and market data:
- **Visa difficulty is 30% higher** than historical average (more rejections in 2024-25)
- **Tech job market is 20% tighter** (layoffs reduced openings)
- **Processing times are 6 months longer** (USCIS backlog)

This is already factored into the probabilities above.
```

### Why LLMs Are Restricted Here
- **Prevents hallucination**: LLM cannot invent probabilities
- **Maintains auditability**: Every number traceable to data
- **Separates concerns**: Logic (data) vs communication (LLM)

---

## Why LLMs Cannot Replace This System

### 1. **LLMs Have No Structural Constraint Knowledge**
- LLM doesn't "know" that H1B requires a bachelor's degree (hallucination risk)
- LDC's Layer 2 encodes this as a hard graph constraint

### 2. **LLMs Cannot Model Uncertainty Honestly**
- LLM gives a single answer ("you should do X")
- LDC shows P10/P50/P90 envelopes

### 3. **LLMs Cannot Learn from Longitudinal Data**
- LLM training is static, doesn't see life trajectories
- LDC trains on PSID, SHARE, job market panels

### 4. **LLMs Cannot Compute Regret or Lock-In**
- LLM can't run counterfactual analysis systematically
- LDC's Layer 4 is purpose-built for this

### 5. **Fine-Tuning Doesn't Fix the Core Problem**
- Fine-tuning = better language, not better reasoning
- LDC separates reasoning (data models) from language (LLM)

---

## Evaluation & Defensibility

### How to Prove the System is Data-Driven

#### 1. **Ablation Study**
```python
# Compare: LDC vs LLM-only vs Human Expert

test_cases = load_test_cases(n=100)  # Real user scenarios

for case in test_cases:
    # LDC prediction
    ldc_outcome = ldc_system.predict(case, horizon_years=5)
    
    # LLM-only prediction
    llm_outcome = ask_gpt4("What will happen if I do X?", case)
    
    # Human expert prediction
    expert_outcome = survey_career_counselor(case)
    
    # Wait 5 years, measure actual outcome
    actual_outcome = case.ground_truth_after_5_years

# Metrics:
# - Calibration: Are P50 predictions actually 50th percentile?
# - Sharpness: How tight are P10-P90 envelopes?
# - Regret accuracy: Did lock-in warnings match reality?
```

#### 2. **Transparency Audit**
```
For every prediction, LDC must provide:
1. Source data: "This probability comes from 237 observations in LinkedIn data"
2. Constraint proof: "This path blocked because: visa_requires(H1B, bachelor_degree) âˆ§ user.has(bachelor) = False"
3. Model version: "Transition model v2.3, trained 2025-08-15"
```

#### 3. **Stress Testing**
```python
# Test: Can LDC catch impossible futures?

impossible_cases = [
    {"age": 45, "goal": "Apply for Canada Express Entry"},  # Age limit 35
    {"degree": None, "goal": "Get US H1B"},  # Requires degree
    {"savings": "$5k", "goal": "EB-5 investor visa"}  # Requires $800k
]

for case in impossible_cases:
    result = ldc_system.predict(case)
    assert result.status == "INFEASIBLE", f"Failed to catch: {case}"
```

### How to Defend Against "ChatGPT Can Do This" Criticism

**Response Script:**
> "ChatGPT can generate *advice*, but it cannot:
> 1. **Prove** its predictions come from data (LDC cites specific datasets)
> 2. **Quantify** uncertainty (LDC gives P10/P50/P90, ChatGPT gives single answer)
> 3. **Detect** impossible futures (LDC has hard constraint graph)
> 4. **Compute** regret (LDC runs counterfactual analysis)
> 5. **Audit** its reasoning (LDC logs every decision step)
>
> ChatGPT is a language model. LDC is decision infrastructure."

---

## Implementation Roadmap

### 3-Month MVP (Hackathon Version)

#### Scope
- **1 decision domain**: Software engineering career (US + India)
- **1 constraint graph**: US/India visa + degree requirements
- **1 trajectory dataset**: Simulated from public H1B data + Glassdoor
- **3 outcome envelopes**: P10/P50/P90 for net worth over 5 years
- **Basic regret analysis**: "If you don't do MS, you lose X in median case"
- **LLM explanation**: GPT-4 translates outputs to plain English

#### Tech Stack
```
Frontend:     React + D3.js (for trajectory visualization)
Backend:      Python FastAPI
Database:     Neo4j (for knowledge graphs)
ML:           scikit-learn (quantile regression), lifelines (survival analysis)
LLM:          OpenAI GPT-4 API (explanation only)
Deployment:   Docker + AWS Lambda
```

#### Deliverables
1. **Web Interface**
   - Structured questionnaire (age, degree, savings, etc.)
   - Upload resume (basic parsing)
   - Select decision: "Do MS in USA" vs "Stay in current role"

2. **Visualization**
   - Interactive graph showing 3 trajectory envelopes
   - Lock-in warnings highlighted in red
   - Regret comparison table

3. **Explanation Panel**
   - LLM-generated plain English summary
   - Data sources cited (e.g., "Based on 1,247 H1B cases")

#### MVP Limitations (Clearly Stated)
- Only software engineering domain
- Only US/India geographies
- Simulated data (not real longitudinal survey)
- No real-time RAG (context modifiers hardcoded)

---

### 2-Year Production System

#### Scope
- **10+ decision domains**: Career, education, finance, migration, entrepreneurship, healthcare
- **Global coverage**: 20+ countries, 100+ visa types
- **Real datasets**: PSID, SHARE, IHDS, LinkedIn, H1B records
- **RAG integration**: Live policy updates, economic indicators
- **Advanced risk analysis**: Time-decay regret, sunk cost analysis, portfolio optimization

#### Tech Stack (Upgraded)
```
Frontend:     Next.js + React + D3.js
Backend:      Python (FastAPI) + Rust (graph computation)
Database:     Neo4j (constraints) + PostgreSQL (user data) + Pinecone (RAG vectors)
ML:           PyTorch (deep survival models), Stan (Bayesian inference)
LLM:          GPT-4 + Claude (ensemble for explanation)
Deployment:   Kubernetes + AWS/GCP
Monitoring:   Datadog (model performance), Arize (ML observability)
```

#### Key Features
1. **Multi-Objective Optimization**
   - User can weight: income, work-life balance, stability, location preference
   - Pareto frontier shows non-dominated paths

2. **Portfolio View**
   - "If you're unsure, split: 60% BigTech, 40% Startup"
   - Risk-adjusted recommendations (like investment portfolios)

3. **Collaborative Mode**
   - Family/spouse can input their constraints
   - Joint optimization (e.g., dual-career couples)

4. **Continuous Learning**
   - User reports outcomes after 1 year
   - Model retrains on new data (privacy-preserving)

---

## Hackathon-Winning Features

### ğŸ† What Makes This a Winning Hackathon Project

#### 1. **Novel Technical Approach** (30% of score)
- âœ… **Not another chatbot**: LLM-constrained architecture
- âœ… **Defensible AI**: Every number traceable to data
- âœ… **Rare technique**: Survival analysis + knowledge graphs in one system

#### 2. **Real-World Impact** (30% of score)
- âœ… **High-stakes domain**: Affects jobs, immigration, life outcomes
- âœ… **Underserved market**: No existing tool quantifies regret or lock-in
- âœ… **Global applicability**: Works for any geography/age/career

#### 3. **Demo Quality** (20% of score)
- âœ… **Visual wow factor**: Interactive trajectory graphs, Pareto frontiers
- âœ… **Live data**: Pull real H1B approval rates, show it updates predictions
- âœ… **Before/After**: Show ChatGPT's vague advice vs LDC's quantified futures

#### 4. **Technical Depth** (20% of score)
- âœ… **Multi-disciplinary**: Combines ML, graph theory, decision science, HCI
- âœ… **Production-ready**: Docker, API, database schema
- âœ… **Scalable**: Architecture supports 10+ domains without rewrite

---

### Feature Checklist for Hackathon Demo

#### Must-Have (Core)
- [x] Structured user input (age, degree, savings, location)
- [x] Constraint graph visualization (Neo4j browser or custom D3)
- [x] Three trajectory envelopes (P10/P50/P90)
- [x] Lock-in detection ("This decision closes off X paths")
- [x] Regret computation ("Declining this costs you $120k at median")
- [x] LLM explanation (GPT-4 translates numbers to English)
- [x] Data source citations ("Based on 1,247 H1B cases, USCIS 2024")

#### Nice-to-Have (Differentiators)
- [x] Live RAG integration (pull H1B approval rates from live API)
- [x] Comparison mode (side-by-side: MS vs Stay vs Startup)
- [x] Reversibility score visualization (0-1 gauge)
- [x] Time-decay regret graph (show sunk cost curve)
- [x] Resume upload + parsing (auto-fill user state)

#### Wow Factor (If Time Permits)
- [ ] Interactive Pareto frontier (drag sliders to weight income vs balance)
- [ ] Scenario stress-testing ("What if recession? What if visa delay?")
- [ ] Collaborative mode (spouse/family can add constraints)
- [ ] Mobile-responsive (judges can test on phone)

---

### Demo Script (5-Minute Pitch)

**[0:00-0:30] Problem Statement**
> "High-stakes life decisionsâ€”career changes, grad school, immigrationâ€”have poor visibility into long-term consequences. Existing tools are advice-based, short-term, and non-quantitative. ChatGPT can give you advice, but it can't compute regret or prove its predictions."

**[0:30-1:30] Solution Overview**
> "Life Decision Coach is decision infrastructure, not advice. It computes reachable future life trajectories using:
> - Real historical data (PSID, H1B records)
> - Hard structural constraints (visa rules, degree requirements)
> - Empirical transition models (survival analysis, Bayesian inference)
> - Uncertainty-aware outcome envelopes (P10/P50/P90, not random simulation)
>
> LLMs are used ONLY for explanation, never for decision logic."

**[1:30-3:00] Live Demo**
> "Let me show you. I'm a 28-year-old software engineer in Mumbai, considering an MS in the US.
>
> [Input data] â†’ LDC checks constraint graph â†’ H1B requires bachelor's (I have it, so feasible)
>
> [Trajectory computation] â†’ Shows 3 envelopes:
> - P10: Visa rejected, net worth $45k in 10 years
> - P50: H1B approved, L5 SWE, net worth $180k
> - P90: FAANG L6, Green Card, net worth $420k
>
> [Regret analysis] â†’ If I decline, I lose $120k at median, $350k at P90
>
> [Lock-in] â†’ This path closes off: Indian startup founder, early retirement
>
> [Explanation] â†’ GPT-4 translates this into plain English, citing data sources"

**[3:00-4:00] Why This Wins**
> "This is not a chatbot. Every prediction is traceable to data. Every constraint is auditable. Every probability is calibrated.
>
> Compare to ChatGPT: [Show side-by-side]
> - ChatGPT: 'I think you should do an MS, it opens doors' (vague, no numbers)
> - LDC: 'Median outcome +$120k, but 10% chance visa rejected, locks out startup path' (quantified, honest)
>
> This is startup-scale: subscription model for career changers, B2B for immigration lawyers, enterprise for HR departments."

**[4:00-5:00] Technical Depth**
> "Architecture:
> - Layer 1: Structured ingestion (no free-text)
> - Layer 2: Constraint graph (Neo4j)
> - Layer 3: Trajectory models (survival analysis + quantile regression)
> - Layer 4: Regret computation (counterfactual analysis)
> - Layer 5: LLM explanation (GPT-4, constrained prompts)
>
> This combines ML, graph theory, decision science, and HCI. Production-ready with Docker, API, database schema. Scales to 10+ domains."

---

## Who Would Pay for This?

### B2C (Direct to Consumer)
- **Target**: Career changers, immigration seekers, grad school applicants
- **Pricing**: $29/month or $199/lifetime
- **TAM**: 50M+ people considering major life decisions annually

### B2B (Business Model)
1. **Immigration Lawyers**: $500/month for client decision dashboard
2. **Career Coaches**: $99/month for white-label platform
3. **Universities**: $10k/year for student career planning

### Enterprise
- **HR Departments**: "Retention risk analysis" â€“ predict which employees will leave
- **Consulting Firms**: "Career path optimizer" for consultant retention

---

## Summary: LDC vs Everything Else

| Feature | LDC | ChatGPT | Career Coach | Traditional MBA |
|---------|-----|---------|--------------|-----------------|
| **Decision Logic** | Data-driven | LLM reasoning | Experience | Case studies |
| **Uncertainty** | P10/P50/P90 | Single answer | Vague | Single scenario |
| **Constraints** | Hard graph | Hallucination risk | Ad-hoc | None |
| **Regret Analysis** | Quantified | Not available | Qualitative | Not available |
| **Auditability** | Full trace | Black box | None | None |
| **Scalability** | Automated | Needs prompting | 1-on-1 only | Classroom only |
| **Cost** | $29/month | $20/month | $200/hr | $200k degree |

---

## Conclusion

**Life Decision Coach** is not advice. It's infrastructure.

It doesn't tell you what to do. It shows you what's **reachable**, what's **impossible**, what you'll **lock out**, and what you might **regret**.

It's data-first, constraint-aware, uncertainty-honest, and LLM-restricted.

It's the system that should exist before anyone makes a high-stakes life decision.

---

> [!NOTE]
> **For hackathon judges**: This is not vaporware. The 3-month MVP is buildable with existing tools (Neo4j, scikit-learn, lifelines, GPT-4). The 2-year production system has clear scaling path. The market is underserved and TAM is massive.

> [!IMPORTANT]
> **Core Philosophy**: Treat life decisions like engineering problemsâ€”model the system, quantify uncertainty, optimize under constraints. Don't rely on advice. Build decision infrastructure.




